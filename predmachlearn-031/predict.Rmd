---
title: "Machine Learning Prediction"
date: "August 3, 2015"
output:
  html_document:
    keep_md: true
---

```{r, echo=TRUE}
## Including the required R packages.
packages <- c('nnet', 'e1071')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
  install.packages(setdiff(packages, rownames(installed.packages())))  
}

library(nnet)
library(e1071)

tidy <- function(data, isTraining = TRUE) {
  # Remove rows with new_window == 'yes' (about 406 rows), as these contain summary information.
  data2 <- data[data$new_window == 'no', ]
  data2$new_window <- NULL
  
  # Remove columns (about 60) with all NA values.
  data3 <- data2[, colSums(is.na(data2)) < nrow(data2)]
  
  if (isTraining == TRUE) {
    # Randomize the rows for training and cross-validation.
    data3 <- data3[sample(nrow(data3)), ]
  }
  
  # Split into training 60% and cross-validation 40% set.
  training <- head(data3, nrow(data3) * .6)
  cv <- tail(data3, nrow(data3) * .4)

  list(training, cv)
}

# Download training data, if it does not exist.
fileName <- 'pml-training.csv';
if (!file.exists(fileName)) {
  download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', fileName, method='auto')
}

# Read csv file.
data <- read.csv(fileName, na.strings=c('', 'NA'), stringsAsFactors=TRUE)

# Create tidy training and cross-validation set.
t <- tidy(data)
training <- t[[1]]
cv <- t[[2]]

# Analysis 1: Create neural network model on training set. Input: 5 nodes, Hidden: 8 nodes, Output: 1 node.
#mod1 <- nnet(classe ~ . - num_window - cvtd_timestamp - raw_timestamp_part_1 - raw_timestamp_part_2 - user_name - X, data = training, maxit = 10000, size = 12)

# Predict results of nnet on training.
#training$y1 <- predict(mod1, training, type='class')
#trainError <- sum(training$classe != training$y1) / nrow(training)
#trainAccuracy <- 1 - trainError

# Predict results of nnet on cv.
#cv$y1 <- predict(mod1, cv, type='class')
#cvError <- sum(cv$classe != cv$y1) / nrow(cv)
#cvAccuracy <- 1 - cvError

# Analysis 2: SVM
# First, determine optimal svm settings by doing an SVM grid search on a subset of the training data (doing a grid search on the full training set would take too long). epsilon = from 0 to 1, step 0.3. cost = 2^2, 2^3 ... 2^6.
tuneResult <- tune(svm, classe ~ . - num_window - cvtd_timestamp - raw_timestamp_part_1 - raw_timestamp_part_2 - user_name - X, data = head(training, 1000), ranges = list(epsilon = seq(0,1,0.3), cost = 2^(2:6)))
plot(tuneResult)

# Get the best svm and take its cost and gamma values to run on the full training data set.
tunedModel <- tuneResult$best.model
cost <- tuneResult$best.model$cost
gamma <- tuneResult$best.model$gamma

# Create an svm model on training set.
mod2 <- svm(classe ~ . - num_window - cvtd_timestamp - raw_timestamp_part_1 - raw_timestamp_part_2 - user_name - X, training, cost = cost, gamma = gamma)

# Predict results of svm on training.
training$y2 <- predict(mod2, training)
trainError <- sum(training$classe != training$y2) / nrow(training)
trainAccuracy <- 1 - trainError

# Predict results of svm on cv.
cv$y2 <- predict(mod2, cv)
cvError <- sum(cv$classe != cv$y2) / nrow(cv)
cvAccuracy <- 1 - cvError

# Download test data, if it does not exist.
fileName <- 'pml-testing.csv';
if (!file.exists(fileName)) {
  download.file('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', fileName, method='auto')
}

# Read csv file.
data2 <- read.csv(fileName, na.strings=c('', 'NA'), stringsAsFactors=TRUE)

# Create tidy test set.
t <- tidy(data2, FALSE)
test <- t[[1]]
test <- rbind(test, t[[2]])

# Predict results of svm on test.
test$y <- predict(mod2, test)
```